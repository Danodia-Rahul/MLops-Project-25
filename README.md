# ğŸ§  Insurance Premium Prediction (MLOps Project)

This project is built as part of the **#mlopszoomcamp** challenge. The goal is to predict insurance premiums based on features such as age, income, occupation, and credit score.

We follow a practical MLOps pipeline using tools like **MLflow**, **XGBoost**, **Prefect**, and **Docker**. The model was trained in a separate environment (e.g., Colab) and the trained model artifacts are reused here for fast inference.

This repository focuses on:
- Serving a trained model using **Flask** API
- Building a **Dockerized** microservice for prediction
- Running and testing the model inference workflow


## ğŸ› ï¸ Tech Stack

- **Language:** Python 3.11  
- **ML Framework:** XGBoost  
- **Cloud:** AWS  
- **Orchestration:** Prefect  
- **Web Framework:** Flask + Gunicorn  
- **Containerization:** Docker  
- **Experiment Tracking:** MLflow  
- **Version Control:** Git & GitHub

## ğŸ“ Project Structure

MLOPS-PROJECT-25
â”‚
â”œâ”€â”€ Data/
â”‚ â””â”€â”€ train.csv â† Training data 
â”‚
â”œâ”€â”€ Deployment/ 
â”‚ â”œâ”€â”€ app.py 
â”‚ â”œâ”€â”€ predict.py
â”‚ â”œâ”€â”€ test.py 
â”‚ â””â”€â”€ Dockerfile 
â”‚
â”œâ”€â”€ MLFlow/ 
â”‚ â”œâ”€â”€ colab_mlflow_tracking.ipynb
â”‚ â””â”€â”€ mlflow_tracking_script.py
â”‚
â”œâ”€â”€ Models/ 
â”‚ â”œâ”€â”€ model.xgb
â”‚ â””â”€â”€ preprocessor.bin
â”‚
â”œâ”€â”€ Prefect/ 
â”‚ â”œâ”€â”€ prefect_flow.py
â”‚ â””â”€â”€ prefect_setup.sh
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pylintrc
â”œâ”€â”€ .prefectignore
â”œâ”€â”€ Pipfile / Pipfile.lock 
â”œâ”€â”€ prefect.yaml 
â”œâ”€â”€ pyproject.toml 
â”œâ”€â”€ req.txt 
â””â”€â”€ README.md 


## âš™ï¸ Setup & Installation

### 1. ğŸš€ Clone the Repository

```bash
git clone https://github.com/your-username/mlops-25-project.git
cd mlops-25-project


### 2. ğŸ³ Build & Run with Docker

We are using a Dockerized Flask application served via Gunicorn.

#### âœ… Build the Docker Image

```bash
docker build -t premium-predictor -f Deployment/Dockerfile .


#### âœ… Run the Container

```bash
docker run -p 9696:9696 premium-predictor


Make sure Docker is running in the background.

### On Linux

Start Docker with the following command:

```bash
sudo service docker start

Once everything is set up, the service will be available at:

```bash
http://localhost:9696/predict

## ğŸ§ª Running a Test Inference

Run the following command:

```bash
python Deployment/test.py

You should see a response like:

### ğŸ” Inference API Details

**Endpoint**  
**URL**: `/predict`  
**Method**: `POST`  
**Content-Type**: `application/json`

---

**ğŸ“¥ Example Input**

```json
[
  {
    "Age": 30.0,
    "Annual Income": 32000.0,
    "Number of Dependents": 3.0,
    "Occupation": "Employed",
    "Credit Score": 690.0,
    "Property Type": "House"
  }
]

**ğŸ“¤ Example Output**

```json
{
  "prediction": [594.72]
}

## ğŸ”„ Prefect Flow

To orchestrate the inference workflow using Prefect:

1. **Start the Prefect Server**  
   Make sure the Prefect server is running in a separate terminal:

   ```bash
   prefect server start


## âš™ï¸ Prefect Setup

Run the following command to set up Prefect:

```bash
bash Prefect/prefect_setup.sh

## âš™ï¸ Prefect Setup Details

Running the setup script will:

- âœ… Create a **work pool**
- âœ… Create a **deployment**
- âœ… Start a **worker** from the work pool

Once completed, you can navigate to the **Prefect UI** and start a flow using the **Quick Run** option from the created deployment.
